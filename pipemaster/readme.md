This directory holds the information for running the program pipemaster to perform demographic modeling. For this manuscript, we designed a custom pipeline for creating the demographic models, tweaking/optimizing prior distributions for key parameters, and rigorously implementing the simulation and model selection approach. The details of each of these steps are outlined below:

## 1: Setting up demographic models
- The approach to setting up models in pipemaster can be challenging. Essentially the best approach is just to familiarize yourself with the overall structure of the model object, and manipulate it manually in R until you have the exact model and set of priors you want. Regularly using the PlotModel() function from the PipeMaster R package is key to making sure that you are actually setting up the model you're interested in correctly. For this dataset, example code outlining how I set up the four models I was interested in can be found at: [https://devonderaad.github.io/phil.dicaeum.rad/pipemaster/set.up.models.html](https://devonderaad.github.io/phil.dicaeum.rad/pipemaster/set.up.models.html). Each of the models used in the manuscript can be directly downloaded from this repository (files 'm1.txt', 'm2.txt', 'm3.txt', and 'm4.txt'). You can read these models into R using the dget() command, and modify them to fit your datasets or use them to validate our results. The file '2188.whitelist.txt' has the list of loci we used, and 'PM.popmap.txt' has the list of all samples included. The file 'popassign.csv' assigns the 58 individuals included into the analysis into the three distinct populations we modeled. The exact empirical data structure was replicated for each of these models using the PipeMaster function, according to the 2188 individual fasta loci used as input for this analysis. We also calculated the empirical (i.e., observed) summary statistics for this dataset based on these same 2188 fasta loci including invariant sites. A directory containing all 2188 files can be accessed by downloading and unzipping the file 'haplotype.fastas.zip'.

## 2: Optimizing/tweaking prior distributions
- After setting up these four models, we performed a preliminary round of parameter optimization, where we assessed the fit of our models to the empirical dataset and iteratively tweaked the parameters Ne and divergence time in an attempt to center our emprical dataset within the parameter space of the simulated models using the parameters Fst, Pi, and # of segregating sites to determine best fit. We observed that these efforts improved the fit of our models to the data, yet some parameters were still poorly estimated, with the range of all four models well outside of our observed data. To ameliorate this, we performed a one-sample t-test to generate a t-statistic quantifying the likelihood of our observed data coming from the set of models for all 82 estimated parameters. We then used this t-statistic value to remove the bottom quartile (here 20 variables) of variables that predicted our observed data most poorly. We then removed any of the remaining 62 variables that were > 95% correlated with another variable to prevent over-fitting, giving us a final dataset of ~20-30 informative, relatviely predictive variables. We then visualized parameter space for the simulated models and the observed data, to ensure that we had a optimized our priors and this variable selection protocol to generate a set of simulations that were informative for our empirical scenario. Finally, we performed cross-validation to also ensure that the four simulated models were themselves differentiable from one another with relatively high accuracy. Specific model parameters (in our case divergence time priors) can be tweaked if your models are not differentiable. For instance, we modified our divergence time priors to specify non-overlapping time windows for the two coalescence events in the bifurcating models (1 & 2) which helped differentiate them in parameter space from the tri-furcation models by making the average time between coalescence events greater (versus specifying the same prior for both but adding a rule enforcing the tree topology, which resulted in a greater proportion of near-polytomy rapid divergence scenarios). Of course this was only clear because of extensive parameter exploration. Once you have this entire approach dialed in and feel good about the exact priors in each simulation, you can finally move on to step 3 where you run the entire protocol with sufficient sample size. This entire protocol for the *Dicaeum* dataset is available to view at: [[https://devonderaad.github.io/phil.dicaeum.rad/pipemaster/preliminary.parameter.optimization.html](https://devonderaad.github.io/phil.dicaeum.rad/pipemaster/preliminary.parameter.optimization.html).

## 3: Running the entire simulation and model selection process in replicate
- We then used these optimized models with dialed in parameters to simulate 50 thousand unique iterations of each model. We then read in all 200K simulations (50K * 4 models) and determined the ideal model fit using a neural network trained on 75% of the input dataset (25% left-out to be treated as test data). Because the performance of the neural net can vary widely based on idiosyncracies in the randomly sampled proportion of data used for testing, we repeated this neural-net training, testing, and model prediction procedure 100 times, and recording which model was selected each time. This entire process (200K simulations plus 100 replicated neural-net model selection iterations) was repeated 5 times to ensure convergence on a single model as generally producing data most similar to our observed, regardless of the vagaries of individual simulaitons or the random downsampling associated with the training process for the neural-net. The results from each replicate can be viewed in the files called 'rep*.neuralnet.model.selection.csv' (there is one for each of the five replicates). We report the number of times each of the four models was selected as most likely to have generated the observed data out of these 500 total iterations (100 randomly trained neural nets * 5 replicates of the total procedure) in the summary file called [mean.support.across.all.runs.csv](https://github.com/DevonDeRaad/phil.dicaeum.rad/blob/main/pipemaster/mean.support.across.all.runs.csv). The entire procedure of this final set of runs, including detailed code and output visualizations, can be viewed at: [[https://devonderaad.github.io/phil.dicaeum.rad/pipemaster/full.pipemaster.run.html](https://devonderaad.github.io/phil.dicaeum.rad/pipemaster/full.pipemaster.run.html).
